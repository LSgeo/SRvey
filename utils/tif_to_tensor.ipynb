{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert raster to tiled ML-ready data\n",
    "\n",
    "ers and tif files are common for geoscience. We need a quick way to tile these and save them to our standard numpy tile dataset.\n",
    "\n",
    "This notebook defines a function which:\n",
    "1. converts a tif to a pytorch tensor\n",
    "1. pads any nan areas with reflection padding \n",
    "1. unfolds the tensor in each direction (read torch .fold() / .unfold())\n",
    "1. stacks those into a \"batch\" of tiles\n",
    "1. generates a selection of indices for validation and training data\n",
    "1. saves each to a seperate train/val folder\n",
    "1. Generates some QA/QC figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import logging\n",
    "from pathlib import Path\n",
    "\n",
    "import colorcet as cc\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import rasterio as rio\n",
    "import torch\n",
    "import tifffile\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def img_to_tiles(\n",
    "    lr_raster_path,\n",
    "    hr_raster_path,\n",
    "    lr_out_prefix=\"lr_tiles\",\n",
    "    hr_out_prefix=\"hr_tiles\",\n",
    "    lr_s=32,  # lr tile res\n",
    "    hr_s=128,  # hr tile res\n",
    "    nan_val=-999_999,\n",
    "    ext=\"npy\",\n",
    "    norm=False,\n",
    "    single_output_folder=False,\n",
    "):\n",
    "\n",
    "    \"\"\"\n",
    "    Specify a LR and HR image pair. This will tile and save them to tensor\n",
    "    arrays or image tiles ready to load in pytorch.\n",
    "\n",
    "    Normalisation is hacky and requires further work.\n",
    "\n",
    "    \"\"\"\n",
    "    if norm:\n",
    "        max_ = 1000  # Aus Magmap v7 histogram arbitrary clip/stats\n",
    "        min_ = -1000\n",
    "        mean_ = 0\n",
    "        std_ = 250\n",
    "\n",
    "        # norm = lambda i: (i + 4403.574) / 18454.907 # values from training\n",
    "        # unnorm = lambda i: (i * 18454.907) - 4403.574\n",
    "\n",
    "        norm = lambda i: (i - min_) / (max_ - min_)  # values from training\n",
    "        unnorm = lambda i: (i * (max_ - min_)) + min_\n",
    "\n",
    "        print(f\"Using {max_=}, {min_=}, {mean_=}, {std_=} for min max norm\")\n",
    "\n",
    "    else:\n",
    "        norm = unnorm = lambda i: i  # NULL OP\n",
    "        print(\"Not Normalising ...\")\n",
    "        # print(\"Using temp /8 norm for PPRDC, which outputs in +- 7. ish\")\n",
    "\n",
    "    if Path(lr_raster_path).suffix == \".tif\":\n",
    "        lr = tifffile.imread(lr_raster_path)\n",
    "        hr = tifffile.imread(hr_raster_path)\n",
    "    elif Path(lr_raster_path).suffix == \".ers\":\n",
    "        lr = np.array(rio.open(lr_raster_path).read(1))\n",
    "        hr = np.array(rio.open(hr_raster_path).read(1))\n",
    "\n",
    "    lr[lr == nan_val] = np.nan\n",
    "    hr[hr == nan_val] = np.nan\n",
    "\n",
    "    lr_tensor = torch.as_tensor(norm(lr), dtype=torch.float32).unsqueeze(0)\n",
    "    hr_tensor = torch.as_tensor(norm(hr), dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "    # lr_original_extent = lr_tensor.shape\n",
    "    # hr_original_extent = hr_tensor.shape\n",
    "\n",
    "    # Expand raster size to a common multiple of lr size (which is a multiple of hr)\n",
    "\n",
    "    # padded_w = (lr_tensor.shape[0] + (lr_s - (lr_tensor.shape[0] % lr_s)))\n",
    "    # padded_h = (lr_tensor.shape[1] + (lr_s - (lr_tensor.shape[1] % lr_s)))\n",
    "\n",
    "    lr_tensor = torch.nn.functional.pad(\n",
    "        lr_tensor,\n",
    "        (\n",
    "            0,\n",
    "            (lr_s - (lr_tensor.shape[1] % lr_s)),\n",
    "            0,\n",
    "            (lr_s - (lr_tensor.shape[0] % lr_s)),\n",
    "        ),\n",
    "        mode=\"reflect\",\n",
    "    )\n",
    "    hr_tensor = torch.nn.functional.pad(\n",
    "        hr_tensor,\n",
    "        (\n",
    "            0,\n",
    "            (hr_s - (hr_tensor.shape[1] % hr_s)),\n",
    "            0,\n",
    "            (hr_s - (hr_tensor.shape[0] % hr_s)),\n",
    "        ),\n",
    "        mode=\"reflect\",\n",
    "    )\n",
    "\n",
    "    hr_tensor = hr_tensor[0]\n",
    "    lr_tensor = lr_tensor[0]\n",
    "\n",
    "    # Math to determine and execute the tiling process\n",
    "    lr_tiles_per_row = lr_tensor.shape[1] // lr_s\n",
    "    hr_tiles_per_row = hr_tensor.shape[1] // hr_s\n",
    "    lr_tiles_per_column = lr_tensor.shape[0] // lr_s\n",
    "    hr_tiles_per_column = hr_tensor.shape[0] // hr_s\n",
    "\n",
    "    lr_patches = lr_tensor.unfold(0, lr_s, lr_s).unfold(1, lr_s, lr_s)\n",
    "    hr_patches = hr_tensor.unfold(0, hr_s, hr_s).unfold(1, hr_s, hr_s)\n",
    "\n",
    "    lr_patches = lr_patches.contiguous().view(\n",
    "        lr_tiles_per_row * lr_tiles_per_column, -1, lr_s, lr_s\n",
    "    )\n",
    "    hr_patches = hr_patches.contiguous().view(\n",
    "        hr_tiles_per_row * hr_tiles_per_column, -1, hr_s, hr_s\n",
    "    )\n",
    "\n",
    "    print(f\"{len(lr_patches)=}\")\n",
    "    print(f\"{len(hr_patches)=}\")\n",
    "\n",
    "    print(f\"{lr_patches.shape=}\")\n",
    "    print(f\"{hr_patches.shape=}\")\n",
    "\n",
    "    #  Drop tiles that contain mostly/any nan values, convert rest to some value\n",
    "    allowed_nan_pct = 0.05\n",
    "    nan_fill_val = mean_  # nan_val\n",
    "    valid_mask = (\n",
    "        torch.count_nonzero(lr_patches.isnan(), dim=(2, 3)) / lr_s ** 2\n",
    "    ) <= allowed_nan_pct\n",
    "\n",
    "    lr_patches_masked = lr_patches[valid_mask]\n",
    "    hr_patches_masked = hr_patches[valid_mask]  # HR and LR indices need to match\n",
    "    print(\n",
    "        f\"Dropped {sum(~valid_mask).item()} mostly NaN tiles (> {allowed_nan_pct*100}% nan)\"\n",
    "    )\n",
    "    # nan_val = torch.tensor(0)  # np.nanmean(hr_patches_masked))\n",
    "    # lr_patches_masked[lr_patches_masked == torch.nan] = nan_val\n",
    "    # hr_patches_masked[hr_patches_masked == torch.nan] = nan_val\n",
    "    hr_patches_masked = torch.nan_to_num(hr_patches_masked, nan=nan_fill_val)\n",
    "    lr_patches_masked = torch.nan_to_num(lr_patches_masked, nan=nan_fill_val)\n",
    "    print(f\"Reverted any NaNs in remaining tiles to {nan_val}\")\n",
    "\n",
    "    ## PPDRC handling\n",
    "    # # Same for excessive zeros, included at border is Nan value is 0.\n",
    "    # allowed_zero_pct = 0.10\n",
    "    # print(f\"{(lr_patches_masked == 0).shape=}\")\n",
    "\n",
    "    # valid_mask = (\n",
    "    #     torch.count_nonzero(lr_patches_masked == 0, dim=(1, 2)) / lr_s ** 2\n",
    "    # ) <= allowed_zero_pct\n",
    "    # lr_patches_masked = lr_patches_masked[valid_mask]\n",
    "    # hr_patches_masked = hr_patches_masked[valid_mask]  # HR and LR indices need to match\n",
    "    # print(\n",
    "    #     f\"Dropped {sum(~valid_mask).item()} mostly zero tiles (> {allowed_zero_pct*100}% Zero valued)\"\n",
    "    # )\n",
    "\n",
    "    # Random split the train/val tiles for this dataset\n",
    "    from numpy.random import default_rng\n",
    "\n",
    "    rng = default_rng(seed=21)\n",
    "\n",
    "    val_pct = 0.15  # 15% Val, 85% Train\n",
    "    val_num = int(np.round(val_pct * len(lr_patches_masked)))  # len(lr_patches)\n",
    "    val_indices = sorted(\n",
    "        rng.choice(len(lr_patches_masked), size=val_num, replace=False)\n",
    "    )\n",
    "    print(f\"{val_indices=}\")\n",
    "\n",
    "    train_indices = [i for i in range(len(lr_patches_masked)) if i not in val_indices]\n",
    "    print(f\"{train_indices=}\")\n",
    "    print(\n",
    "        f\"There are {sum(i in train_indices for i in val_indices)} val indices in your train indices :)\"\n",
    "    )\n",
    "\n",
    "    lr_patches_train = lr_patches_masked[train_indices].numpy().astype(np.float32)\n",
    "    hr_patches_train = hr_patches_masked[train_indices].numpy().astype(np.float32)\n",
    "    lr_patches_val = lr_patches_masked[val_indices].numpy().astype(np.float32)\n",
    "    hr_patches_val = hr_patches_masked[val_indices].numpy().astype(np.float32)\n",
    "\n",
    "    print(f\"{len(lr_patches_train)=}\")\n",
    "    print(f\"{len(lr_patches_val)=}\")\n",
    "\n",
    "    print(\"None of these values should show nans!\")\n",
    "    print(f\"{np.min(lr_patches_train)=}\")\n",
    "    print(f\"{np.max(lr_patches_train)=}\")\n",
    "    print(f\"{np.mean(lr_patches_train)=}\")\n",
    "    print(f\"{np.std(lr_patches_train)=}\")\n",
    "    # print(f\"{np.nanmin(hr_patches_train)=}\")\n",
    "    # print(f\"{np.nanmax(hr_patches_train)=}\")\n",
    "\n",
    "    if single_output_folder:\n",
    "        single_output_folder = Path(single_output_folder)\n",
    "        train_dir = single_output_folder / \"train\"\n",
    "        val_dir = single_output_folder / \"val\"\n",
    "    else:\n",
    "        train_dir = Path(hr_raster_path).parent / ext / \"train\"\n",
    "        val_dir = Path(hr_raster_path).parent / ext / \"val\"\n",
    "\n",
    "    (train_dir / \"hr\").mkdir(parents=True, exist_ok=True)\n",
    "    (train_dir / \"lr\").mkdir(parents=True, exist_ok=True)\n",
    "    (val_dir / \"hr\").mkdir(parents=True, exist_ok=True)\n",
    "    (val_dir / \"lr\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    hr_file_name = f\"hr/{Path(hr_raster_path).stem}_hr\"\n",
    "    lr_file_name = f\"lr/{Path(lr_raster_path).stem}_lr\"\n",
    "\n",
    "    if \"npy\" in ext:\n",
    "        np.save(train_dir / f\"{hr_file_name}.{ext}\", hr_patches_train)\n",
    "        np.save(train_dir / f\"{lr_file_name}.{ext}\", lr_patches_train)\n",
    "        np.save(val_dir / f\"{hr_file_name}.{ext}\", hr_patches_val)\n",
    "        np.save(val_dir / f\"{lr_file_name}.{ext}\", lr_patches_val)\n",
    "\n",
    "    if \"tif\" in ext:\n",
    "        for i in range(len(lr_patches_train)):\n",
    "            tifffile.imsave(\n",
    "                train_dir / f\"{hr_file_name}_{i}.{ext}\", hr_patches_train[i]\n",
    "            )\n",
    "            tifffile.imsave(\n",
    "                train_dir / f\"{lr_file_name}_{i}.{ext}\", lr_patches_train[i]\n",
    "            )\n",
    "\n",
    "        for i in range(len(lr_patches_val)):\n",
    "            tifffile.imsave(val_dir / f\"{hr_file_name}_{i}.{ext}\", hr_patches_val[i])\n",
    "            tifffile.imsave(val_dir / f\"{lr_file_name}_{i}.{ext}\", lr_patches_val[i])\n",
    "\n",
    "        print(hr_patches_val[i].shape)\n",
    "        print(lr_patches_val[i].shape)\n",
    "\n",
    "    print(f\"\\nSaved to {val_dir.parent.absolute()}\")\n",
    "\n",
    "    return lr_patches, hr_patches, val_indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Luke\\data\\Paper_2\\P578\n",
      "Using max_=1000, min_=-1000, mean_=0, std_=250 for min max norm\n",
      "len(lr_patches)=99\n",
      "len(hr_patches)=99\n",
      "lr_patches.shape=torch.Size([99, 1, 64, 64])\n",
      "hr_patches.shape=torch.Size([99, 1, 256, 256])\n",
      "Dropped 0 mostly NaN tiles (> 5.0% nan)\n",
      "Reverted any NaNs in remaining tiles to -999999\n",
      "val_indices=[8, 23, 25, 27, 31, 33, 41, 53, 58, 59, 63, 67, 93, 94, 97]\n",
      "train_indices=[0, 1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 24, 26, 28, 29, 30, 32, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 54, 55, 56, 57, 60, 61, 62, 64, 65, 66, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 95, 96, 98]\n",
      "There are 0 val indices in your train indices :)\n",
      "len(lr_patches_train)=84\n",
      "len(lr_patches_val)=15\n",
      "None of these values should show nans!\n",
      "np.min(lr_patches_train)=-0.064983584\n",
      "np.max(lr_patches_train)=1.4908749\n",
      "np.mean(lr_patches_train)=0.46885294\n",
      "np.std(lr_patches_train)=0.1029138\n",
      "(256, 256)\n",
      "(64, 64)\n",
      "\n",
      "Saved to C:\\Luke\\data\\Paper_2\\lr64_combined\n",
      "C:\\Luke\\data\\Paper_2\\P681\n",
      "Using max_=1000, min_=-1000, mean_=0, std_=250 for min max norm\n",
      "len(lr_patches)=90\n",
      "len(hr_patches)=90\n",
      "lr_patches.shape=torch.Size([90, 1, 64, 64])\n",
      "hr_patches.shape=torch.Size([90, 1, 256, 256])\n",
      "Dropped 0 mostly NaN tiles (> 5.0% nan)\n",
      "Reverted any NaNs in remaining tiles to -999999\n",
      "val_indices=[7, 21, 23, 25, 28, 30, 37, 38, 48, 54, 58, 60, 86, 87]\n",
      "train_indices=[0, 1, 2, 3, 4, 5, 6, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 22, 24, 26, 27, 29, 31, 32, 33, 34, 35, 36, 39, 40, 41, 42, 43, 44, 45, 46, 47, 49, 50, 51, 52, 53, 55, 56, 57, 59, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 88, 89]\n",
      "There are 0 val indices in your train indices :)\n",
      "len(lr_patches_train)=76\n",
      "len(lr_patches_val)=14\n",
      "None of these values should show nans!\n",
      "np.min(lr_patches_train)=0.20501657\n",
      "np.max(lr_patches_train)=1.6083877\n",
      "np.mean(lr_patches_train)=0.48287463\n",
      "np.std(lr_patches_train)=0.08153357\n",
      "(256, 256)\n",
      "(64, 64)\n",
      "\n",
      "Saved to C:\\Luke\\data\\Paper_2\\lr64_combined\n",
      "C:\\Luke\\data\\Paper_2\\P738\n",
      "Using max_=1000, min_=-1000, mean_=0, std_=250 for min max norm\n",
      "len(lr_patches)=108\n",
      "len(hr_patches)=108\n",
      "lr_patches.shape=torch.Size([108, 1, 64, 64])\n",
      "hr_patches.shape=torch.Size([108, 1, 256, 256])\n",
      "Dropped 0 mostly NaN tiles (> 5.0% nan)\n",
      "Reverted any NaNs in remaining tiles to -999999\n",
      "val_indices=[8, 12, 25, 28, 29, 34, 36, 44, 45, 58, 63, 64, 69, 73, 101, 102]\n",
      "train_indices=[0, 1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 26, 27, 30, 31, 32, 33, 35, 37, 38, 39, 40, 41, 42, 43, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 59, 60, 61, 62, 65, 66, 67, 68, 70, 71, 72, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 103, 104, 105, 106, 107]\n",
      "There are 0 val indices in your train indices :)\n",
      "len(lr_patches_train)=92\n",
      "len(lr_patches_val)=16\n",
      "None of these values should show nans!\n",
      "np.min(lr_patches_train)=0.31785408\n",
      "np.max(lr_patches_train)=0.59970933\n",
      "np.mean(lr_patches_train)=0.47453824\n",
      "np.std(lr_patches_train)=0.018442953\n",
      "(256, 256)\n",
      "(64, 64)\n",
      "\n",
      "Saved to C:\\Luke\\data\\Paper_2\\lr64_combined\n",
      "C:\\Luke\\data\\Paper_2\\P739\n",
      "Using max_=1000, min_=-1000, mean_=0, std_=250 for min max norm\n",
      "len(lr_patches)=49\n",
      "len(hr_patches)=49\n",
      "lr_patches.shape=torch.Size([49, 1, 64, 64])\n",
      "hr_patches.shape=torch.Size([49, 1, 256, 256])\n",
      "Dropped 19 mostly NaN tiles (> 5.0% nan)\n",
      "Reverted any NaNs in remaining tiles to -999999\n",
      "val_indices=[8, 11, 18, 21]\n",
      "train_indices=[0, 1, 2, 3, 4, 5, 6, 7, 9, 10, 12, 13, 14, 15, 16, 17, 19, 20, 22, 23, 24, 25, 26, 27, 28, 29]\n",
      "There are 0 val indices in your train indices :)\n",
      "len(lr_patches_train)=26\n",
      "len(lr_patches_val)=4\n",
      "None of these values should show nans!\n",
      "np.min(lr_patches_train)=0.0\n",
      "np.max(lr_patches_train)=0.74584705\n",
      "np.mean(lr_patches_train)=0.49828783\n",
      "np.std(lr_patches_train)=0.03544763\n",
      "(256, 256)\n",
      "(64, 64)\n",
      "\n",
      "Saved to C:\\Luke\\data\\Paper_2\\lr64_combined\n"
     ]
    }
   ],
   "source": [
    "# survey_search = \"*.tif\"\n",
    "survey_search = \"P*\"\n",
    "\n",
    "for survey_path in Path(r\"C:\\Luke\\data\\Paper_2\").glob(survey_search):\n",
    "    if not survey_path.is_dir():\n",
    "        continue\n",
    "    print(survey_path)\n",
    "    root = survey_path\n",
    "\n",
    "    # lr_patches, hr_patches, val_indices = tifs_to_tensors(\n",
    "    #     f\"{root}/{survey_name}_x4_200_LR.tif\",\n",
    "    #     f\"{root}/{survey_name}_x1_50_HR.tif\",\n",
    "    #     ext=\"tif\",\n",
    "    #     norm=True,\n",
    "    # )\n",
    "\n",
    "    lr_patches, hr_patches, val_indices = img_to_tiles(\n",
    "        hr_raster_path=f\"{next(root.glob('*1.ers'))}\",\n",
    "        lr_raster_path=f\"{next(root.glob('*4.ers'))}\",\n",
    "        # hr_raster_path=next(survey_path.glob(\"*0200.tif\")),\n",
    "        # lr_raster_path=next(survey_path.glob(\"*0050.tif\")),\n",
    "        ext=\"tif\",\n",
    "        norm=True,\n",
    "        single_output_folder=\"C:/Luke/data/Paper_2/lr64_combined\",\n",
    "        nan_val=-999999,\n",
    "        lr_s=64,  # lr tile res\n",
    "        hr_s=256,  # hr tile res\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_tiles(data_path, index=0, ext=\"np\", s=256):\n",
    "    # if \"np\" in ext:\n",
    "    #     lr_tile = np.load(lr_path)[index][0]\n",
    "    #     hr_tile = np.load(hr_path)[index][0]\n",
    "    # elif \"tif\" in ext:\n",
    "    #     lr_tile = tifffile.imread(f\"{lr_path}\").squeeze()\n",
    "    #     hr_tile = tifffile.imread(f\"{hr_path}\").squeeze()\n",
    "    data_path = Path(data_path)\n",
    "\n",
    "    if \"tif\" in ext:\n",
    "        lr_tile = tifffile.imread(f\"{next(data_path.glob(f'**/lr/{i}.tif'))}\").squeeze()\n",
    "        hr_tile = tifffile.imread(f\"{next(data_path.glob(f'**/hr/{i}.tif'))}\").squeeze()\n",
    "\n",
    "    us = np.array(Image.fromarray(lr_tile).resize((s, s)))\n",
    "\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(us, vmin=hr_tile.min(), vmax=hr_tile.max())\n",
    "    plt.colorbar()\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.imshow(hr_tile)\n",
    "    plt.colorbar()\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.imshow(hr_tile - us, cmap=cc.cm.CET_D7, vmin=-0.5, vmax=0.5)\n",
    "    plt.colorbar()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def ers_to_tifs(survey_path, out_dir=\"\"):\n",
    "#     out_dir = Path(out_dir)\n",
    "#     out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "#     ers_arr = np.array(rio.open(survey_path).read(1))\n",
    "#     tifffile.imsave(out_dir / f\"{survey_path.stem}.tif\", ers_arr)\n",
    "#     print(f'Saved {Path(out_dir / f\"{survey_path.stem}.tif\").absolute()}')\n",
    "\n",
    "\n",
    "# survey_search = \"*4.ers\"\n",
    "# for survey_path in Path(\"C:/Luke/PhD/Oasis Montaj/ArbSR\").glob(survey_search):\n",
    "#     print(survey_path)\n",
    "#     # survey_path\n",
    "#     ers_to_tifs(survey_path, out_dir=\"PPDRC\")\n",
    "\n",
    "# # Test no loss of information:\n",
    "# # np.max(tifffile.imread(next(Path(r\"C:\\Luke\\PhD\\paper2\\SRvey\\utils\\PPDRC\").glob(\"*.tif\"))) - np.array(rio.open(r\"C:\\Luke\\PhD\\Oasis Montaj\\ArbSR\\p681_1.ers\").read(1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Luke\\AppData\\Local\\Temp/ipykernel_13136/1076295010.py:10: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  ims = np.array(ims)[0]\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import tifffile\n",
    "import numpy as np\n",
    "\n",
    "ims = []\n",
    "for im in Path(r\"C:\\Luke\\data\\Paper 2\\PPDRC\\lr32\\train\").glob(\"**\\*.tif\"):\n",
    "    ims.append(tifffile.imread(im))\n",
    "\n",
    "ims = np.array(ims)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.51393044\n",
      "-0.4547087\n",
      "0.056754638\n"
     ]
    }
   ],
   "source": [
    "print(ims.max())\n",
    "print(ims.min())\n",
    "print(ims.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13136/2148174310.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msurvey_path\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mPath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr\"C:\\Luke\\data\\Paper 2\\PPDRC\\lr32\\train\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msurvey_search\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtifffile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msurvey_path\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"**/*0050.tif\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtifffile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msurvey_path\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"**/*0050.tif\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtifffile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msurvey_path\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"**/*0200.tif\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mStopIteration\u001b[0m: "
     ]
    }
   ],
   "source": [
    "survey_search = \"**/*\"\n",
    "\n",
    "for survey_path in Path(r\"C:\\Luke\\data\\Paper 2\\PPDRC\\lr32\\train\").glob(survey_search):\n",
    "    print(tifffile.imread(next(survey_path.glob(\"**/*0050.tif\"))).max())\n",
    "    print(tifffile.imread(next(survey_path.glob(\"**/*0050.tif\"))).max())\n",
    "    print(tifffile.imread(next(survey_path.glob(\"**/*0200.tif\"))).min())\n",
    "    print(tifffile.imread(next(survey_path.glob(\"**/*0200.tif\"))).min())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c2a7693b0d28a18d07a779b8850ec935428aaafb4510b5c22ddb8cee62302900"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('srvey': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
