{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert raster to tiled ML-ready data\n",
    "\n",
    "ers and tif files are common for geoscience. We need a quick way to tile these and save them to our standard numpy tile dataset.\n",
    "\n",
    "This notebook defines functions which:\n",
    "1. convert a tif to a pytorch tensor\n",
    "1. pads any nan areas with reflection padding \n",
    "1. unfolds the tensor in each direction (read torch .fold() / .unfold())\n",
    "1. stacks those into a \"batch\" of tiles\n",
    "1. generates a selection of indices for validation and training data\n",
    "1. saves each to a seperate train/val folder\n",
    "1. Generates some QA/QC figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:2022-04-15 15:48:18,772: \n",
      "#####\n",
      "Starting new log\n",
      "INFO:2022-04-15 15:48:35,912: Note: NumExpr detected 20 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "INFO:2022-04-15 15:48:35,912: NumExpr defaulting to 8 threads.\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(levelname)s:%(asctime)s: %(message)s\",\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.info(\"\\n#####\\nStarting new log\")\n",
    "\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "\n",
    "import colorcet as cc\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import rasterio as rio\n",
    "import torch\n",
    "import tifffile\n",
    "from PIL import Image\n",
    "\n",
    "class Norm:\n",
    "    def __init__(self, t_file_path):\n",
    "        \"\"\" Load a pre-fit sklearn Transformer to normalise input array\n",
    "        \"\"\"\n",
    "\n",
    "        self.transformer = joblib.load(t_file_path)\n",
    "        logger.info(f\"Using {t_file_path} to transform your input array\")\n",
    "    \n",
    "    def transform(self, arr):\n",
    "        og_shape = arr.shape\n",
    "        arr = arr.flatten().reshape(-1, 1)\n",
    "        arr = self.transformer.transform(arr.astype(np.float64))\n",
    "        return arr.reshape(og_shape)\n",
    "\n",
    "# def norm(arr, loc=-88.5, scale=132.4):\n",
    "#     \"\"\" Standardise input data (with Laplace distibution) to a Standard Gaussian distribution\n",
    "#     Australian 80m TMI (continental) fit laplace: loc=-88.5, scale=132.4\"\"\"\n",
    "#     import scipy.stats\n",
    "#     lv = scipy.stats.laplace(loc=loc, scale=scale)\n",
    "#     rv = scipy.stats.norm(loc=0.0, scale=1.0) # \"Standard\" Norm\n",
    "#     return rv.ppf(lv.cdf(arr))\n",
    "\n",
    "\n",
    "def img_to_tiles(\n",
    "    lr_raster_path,\n",
    "    hr_raster_path,\n",
    "    lr_out_prefix=\"lr_tiles\",\n",
    "    hr_out_prefix=\"hr_tiles\",\n",
    "    scale=4,\n",
    "    hr_s=256,\n",
    "    nan_val=-999_999,\n",
    "    ext=\"npy\",\n",
    "    norm=False,\n",
    "    single_output_folder=False,\n",
    "):\n",
    "\n",
    "    \"\"\"\n",
    "    Specify a LR and HR image pair. This will tile and save them to tensor\n",
    "    arrays or image tiles ready to load in pytorch.\n",
    "\n",
    "    Normalisation is hacky and requires further work.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    logging.info(f\"Processing {lr_raster_path}, {hr_raster_path}\")\n",
    "\n",
    "    if Path(lr_raster_path).suffix == \".tif\":\n",
    "        lr = tifffile.imread(lr_raster_path)\n",
    "        hr = tifffile.imread(hr_raster_path)\n",
    "    elif Path(lr_raster_path).suffix == \".ers\":\n",
    "        lr = np.array(rio.open(lr_raster_path).read(1))\n",
    "        hr = np.array(rio.open(hr_raster_path).read(1))\n",
    "\n",
    "    lr[lr == nan_val] = np.nan\n",
    "    hr[hr == nan_val] = np.nan\n",
    "\n",
    "    lr_tensor = torch.as_tensor(norm(lr), dtype=torch.float32).unsqueeze(0)\n",
    "    hr_tensor = torch.as_tensor(norm(hr), dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "    # lr_original_extent = lr_tensor.shape\n",
    "    # hr_original_extent = hr_tensor.shape\n",
    "\n",
    "    # Expand raster size to a common multiple of lr size (which is a multiple of hr)\n",
    "\n",
    "    # padded_w = (lr_tensor.shape[0] + (lr_s - (lr_tensor.shape[0] % lr_s)))\n",
    "    # padded_h = (lr_tensor.shape[1] + (lr_s - (lr_tensor.shape[1] % lr_s)))\n",
    "\n",
    "    lr_s = int(hr_s / scale)\n",
    "\n",
    "    lr_tensor = torch.nn.functional.pad(\n",
    "        lr_tensor,\n",
    "        (\n",
    "            0,\n",
    "            (lr_s - (lr_tensor.shape[1] % lr_s)),\n",
    "            0,\n",
    "            (lr_s - (lr_tensor.shape[0] % lr_s)),\n",
    "        ),\n",
    "        mode=\"reflect\",\n",
    "    )\n",
    "    hr_tensor = torch.nn.functional.pad(\n",
    "        hr_tensor,\n",
    "        (\n",
    "            0,\n",
    "            (hr_s - (hr_tensor.shape[1] % hr_s)),\n",
    "            0,\n",
    "            (hr_s - (hr_tensor.shape[0] % hr_s)),\n",
    "        ),\n",
    "        mode=\"reflect\",\n",
    "    )\n",
    "\n",
    "    hr_tensor = hr_tensor[0]\n",
    "    lr_tensor = lr_tensor[0]\n",
    "\n",
    "    # Math to determine and execute the tiling process\n",
    "    lr_tiles_per_row = lr_tensor.shape[1] // lr_s\n",
    "    hr_tiles_per_row = hr_tensor.shape[1] // hr_s\n",
    "    lr_tiles_per_column = lr_tensor.shape[0] // lr_s\n",
    "    hr_tiles_per_column = hr_tensor.shape[0] // hr_s\n",
    "\n",
    "    assert (\n",
    "        lr_tiles_per_column == hr_tiles_per_column\n",
    "        and lr_tiles_per_row == hr_tiles_per_row\n",
    "    ), f\"These should be equal\"\n",
    "\n",
    "    lr_patches = lr_tensor.unfold(0, lr_s, lr_s).unfold(1, lr_s, lr_s)\n",
    "    hr_patches = hr_tensor.unfold(0, hr_s, hr_s).unfold(1, hr_s, hr_s)\n",
    "\n",
    "    lr_patches = lr_patches.contiguous().view(\n",
    "        lr_tiles_per_row * lr_tiles_per_column, -1, lr_s, lr_s\n",
    "    )\n",
    "    hr_patches = hr_patches.contiguous().view(\n",
    "        hr_tiles_per_row * hr_tiles_per_column, -1, hr_s, hr_s\n",
    "    )\n",
    "\n",
    "    #  Drop tiles that contain mostly/any nan values, convert rest to some value\n",
    "    allowed_nan_pct = 1  # 0.05 TODO currently allow all NaNs for consistent num tiles\n",
    "    nan_fill_val = 0 # norm(np.nanmean(lr_patches))  # nan_val\n",
    "    valid_mask = (\n",
    "        torch.count_nonzero(lr_patches.isnan(), dim=(2, 3)) / lr_s ** 2\n",
    "    ) <= allowed_nan_pct\n",
    "\n",
    "    lr_patches_masked = lr_patches[valid_mask]\n",
    "    hr_patches_masked = hr_patches[valid_mask]  # HR and LR indices need to match\n",
    "    logger.info(\n",
    "        f\"Dropped {sum(~valid_mask).item()} mostly NaN tiles (> {allowed_nan_pct*100}% nan)\"\n",
    "    )\n",
    "    # nan_val = torch.tensor(0)  # np.nanmean(hr_patches_masked))\n",
    "    # lr_patches_masked[lr_patches_masked == torch.nan] = nan_val\n",
    "    # hr_patches_masked[hr_patches_masked == torch.nan] = nan_val\n",
    "\n",
    "    hr_patches = (\n",
    "        torch.nan_to_num(hr_patches_masked, nan=nan_fill_val).numpy().astype(np.float32)\n",
    "    )\n",
    "    lr_patches = (\n",
    "        torch.nan_to_num(lr_patches_masked, nan=nan_fill_val).numpy().astype(np.float32)\n",
    "    )\n",
    "    logger.info(f\"Reverted any NaNs in remaining tiles to {nan_fill_val}\")\n",
    "\n",
    "    logger.info(\"None of these values should show NaN!\")\n",
    "    logger.info(f\"{np.min(lr_patches)=}\")\n",
    "    logger.info(f\"{np.max(lr_patches)=}\")\n",
    "    logger.info(f\"{np.mean(lr_patches)=}\")\n",
    "    logger.info(f\"{np.std(lr_patches)=}\")\n",
    "    # print(f\"{np.nanmin(hr_patches)=}\")\n",
    "    # print(f\"{np.nanmax(hr_patches)=}\")\n",
    "\n",
    "    if single_output_folder:\n",
    "        single_output_folder = Path(single_output_folder)\n",
    "        train_dir = single_output_folder\n",
    "    else:\n",
    "        train_dir = Path(hr_raster_path).parent / ext\n",
    "\n",
    "    (train_dir / \"hr\" / \"1-0\").mkdir(parents=True, exist_ok=True)\n",
    "    (train_dir / \"lr\" / f\"{scale}-0\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    hr_file_name = f\"hr/1-0/{Path(hr_raster_path).stem}\"\n",
    "    lr_file_name = f\"lr/{scale}-0/{Path(lr_raster_path).stem}\"\n",
    "\n",
    "    if \"npy\" in ext:\n",
    "        np.save(train_dir / f\"{hr_file_name}.{ext}\", hr_patches)\n",
    "        np.save(train_dir / f\"{lr_file_name}.{ext}\", lr_patches)\n",
    "\n",
    "    if \"tif\" in ext:\n",
    "        for i in range(len(lr_patches)):\n",
    "            tifffile.imsave(train_dir / f\"{hr_file_name}_{i}.{ext}\", hr_patches[i])\n",
    "            tifffile.imsave(train_dir / f\"{lr_file_name}_{i}.{ext}\", lr_patches[i])\n",
    "\n",
    "        logger.info(hr_patches[i].shape)\n",
    "        logger.info(lr_patches[i].shape)\n",
    "\n",
    "    logger.info(f\"Saved {len(lr_patches)} tiles to {train_dir.absolute()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:2022-04-15 15:50:20,875: Using D:\\luke\\SRvey\\utils\\AUS_MAGPMAP_v7_ONSHORE_QuantileTransformer.joblib to transform your input array\n",
      "INFO:2022-04-15 15:50:20,876: D:\\luke\\data_source\\surveys_line_spacing\\P578\n",
      "INFO:2022-04-15 15:50:20,877: Processing D:\\luke\\data_source\\surveys_line_spacing\\P578\\p578_2.ers, D:\\luke\\data_source\\surveys_line_spacing\\P578\\p578_1.ers\n",
      "INFO:2022-04-15 15:50:21,916: Dropped 0 mostly NaN tiles (> 100% nan)\n",
      "INFO:2022-04-15 15:50:21,926: Reverted any NaNs in remaining tiles to 0\n",
      "INFO:2022-04-15 15:50:21,926: None of these values should show NaN!\n",
      "INFO:2022-04-15 15:50:21,926: np.min(lr_patches)=-3.131595\n",
      "INFO:2022-04-15 15:50:21,926: np.max(lr_patches)=3.0966911\n",
      "INFO:2022-04-15 15:50:21,926: np.mean(lr_patches)=-0.07793587\n",
      "INFO:2022-04-15 15:50:21,926: np.std(lr_patches)=1.0870522\n",
      "INFO:2022-04-15 15:50:22,219: (240, 240)\n",
      "INFO:2022-04-15 15:50:22,219: (120, 120)\n",
      "INFO:2022-04-15 15:50:22,219: Saved 120 tiles to D:\\luke\\data_train\n",
      "INFO:2022-04-15 15:50:22,226: Processing D:\\luke\\data_source\\surveys_line_spacing\\P578\\p578_3.ers, D:\\luke\\data_source\\surveys_line_spacing\\P578\\p578_1.ers\n",
      "INFO:2022-04-15 15:50:23,108: Dropped 0 mostly NaN tiles (> 100% nan)\n",
      "INFO:2022-04-15 15:50:23,120: Reverted any NaNs in remaining tiles to 0\n",
      "INFO:2022-04-15 15:50:23,120: None of these values should show NaN!\n",
      "INFO:2022-04-15 15:50:23,121: np.min(lr_patches)=-3.1035159\n",
      "INFO:2022-04-15 15:50:23,122: np.max(lr_patches)=2.989896\n",
      "INFO:2022-04-15 15:50:23,123: np.mean(lr_patches)=-0.075856656\n",
      "INFO:2022-04-15 15:50:23,125: np.std(lr_patches)=1.0846615\n",
      "INFO:2022-04-15 15:50:25,686: (240, 240)\n",
      "INFO:2022-04-15 15:50:25,686: (80, 80)\n",
      "INFO:2022-04-15 15:50:25,687: Saved 120 tiles to D:\\luke\\data_train\n",
      "INFO:2022-04-15 15:50:25,693: Processing D:\\luke\\data_source\\surveys_line_spacing\\P578\\p578_4.ers, D:\\luke\\data_source\\surveys_line_spacing\\P578\\p578_1.ers\n",
      "INFO:2022-04-15 15:50:26,560: Dropped 0 mostly NaN tiles (> 100% nan)\n",
      "INFO:2022-04-15 15:50:26,573: Reverted any NaNs in remaining tiles to 0\n",
      "INFO:2022-04-15 15:50:26,573: None of these values should show NaN!\n",
      "INFO:2022-04-15 15:50:26,574: np.min(lr_patches)=-2.9385781\n",
      "INFO:2022-04-15 15:50:26,574: np.max(lr_patches)=2.9627995\n",
      "INFO:2022-04-15 15:50:26,575: np.mean(lr_patches)=-0.07459035\n",
      "INFO:2022-04-15 15:50:26,576: np.std(lr_patches)=1.0825081\n",
      "INFO:2022-04-15 15:50:29,069: (240, 240)\n",
      "INFO:2022-04-15 15:50:29,069: (60, 60)\n",
      "INFO:2022-04-15 15:50:29,070: Saved 120 tiles to D:\\luke\\data_train\n",
      "INFO:2022-04-15 15:50:29,075: D:\\luke\\data_source\\surveys_line_spacing\\P681\n",
      "INFO:2022-04-15 15:50:29,076: Processing D:\\luke\\data_source\\surveys_line_spacing\\P681\\p681_2.ers, D:\\luke\\data_source\\surveys_line_spacing\\P681\\p681_1.ers\n",
      "INFO:2022-04-15 15:50:29,941: Dropped 0 mostly NaN tiles (> 100% nan)\n",
      "INFO:2022-04-15 15:50:29,957: Reverted any NaNs in remaining tiles to 0\n",
      "INFO:2022-04-15 15:50:29,957: None of these values should show NaN!\n",
      "INFO:2022-04-15 15:50:29,958: np.min(lr_patches)=-2.646547\n",
      "INFO:2022-04-15 15:50:29,959: np.max(lr_patches)=3.1057427\n",
      "INFO:2022-04-15 15:50:29,960: np.mean(lr_patches)=0.17648327\n",
      "INFO:2022-04-15 15:50:29,964: np.std(lr_patches)=0.79060346\n",
      "INFO:2022-04-15 15:50:30,434: (240, 240)\n",
      "INFO:2022-04-15 15:50:30,435: (120, 120)\n",
      "INFO:2022-04-15 15:50:30,435: Saved 120 tiles to D:\\luke\\data_train\n",
      "INFO:2022-04-15 15:50:30,441: Processing D:\\luke\\data_source\\surveys_line_spacing\\P681\\p681_3.ers, D:\\luke\\data_source\\surveys_line_spacing\\P681\\p681_1.ers\n",
      "INFO:2022-04-15 15:50:31,210: Dropped 0 mostly NaN tiles (> 100% nan)\n",
      "INFO:2022-04-15 15:50:31,221: Reverted any NaNs in remaining tiles to 0\n",
      "INFO:2022-04-15 15:50:31,222: None of these values should show NaN!\n",
      "INFO:2022-04-15 15:50:31,222: np.min(lr_patches)=-2.468032\n",
      "INFO:2022-04-15 15:50:31,222: np.max(lr_patches)=3.0900114\n",
      "INFO:2022-04-15 15:50:31,224: np.mean(lr_patches)=0.17834951\n",
      "INFO:2022-04-15 15:50:31,224: np.std(lr_patches)=0.7897048\n",
      "INFO:2022-04-15 15:50:34,162: (240, 240)\n",
      "INFO:2022-04-15 15:50:34,163: (80, 80)\n",
      "INFO:2022-04-15 15:50:34,163: Saved 120 tiles to D:\\luke\\data_train\n",
      "INFO:2022-04-15 15:50:34,169: Processing D:\\luke\\data_source\\surveys_line_spacing\\P681\\p681_4.ers, D:\\luke\\data_source\\surveys_line_spacing\\P681\\p681_1.ers\n",
      "INFO:2022-04-15 15:50:34,890: Dropped 0 mostly NaN tiles (> 100% nan)\n",
      "INFO:2022-04-15 15:50:34,902: Reverted any NaNs in remaining tiles to 0\n",
      "INFO:2022-04-15 15:50:34,902: None of these values should show NaN!\n",
      "INFO:2022-04-15 15:50:34,903: np.min(lr_patches)=-2.4663517\n",
      "INFO:2022-04-15 15:50:34,904: np.max(lr_patches)=3.0300198\n",
      "INFO:2022-04-15 15:50:34,905: np.mean(lr_patches)=0.17953287\n",
      "INFO:2022-04-15 15:50:34,906: np.std(lr_patches)=0.78785825\n",
      "INFO:2022-04-15 15:50:37,704: (240, 240)\n",
      "INFO:2022-04-15 15:50:37,704: (60, 60)\n",
      "INFO:2022-04-15 15:50:37,706: Saved 120 tiles to D:\\luke\\data_train\n",
      "INFO:2022-04-15 15:50:37,709: D:\\luke\\data_source\\surveys_line_spacing\\P738\n",
      "INFO:2022-04-15 15:50:37,709: Processing D:\\luke\\data_source\\surveys_line_spacing\\P738\\p738_2.ers, D:\\luke\\data_source\\surveys_line_spacing\\P738\\p738_1.ers\n",
      "INFO:2022-04-15 15:50:38,695: Dropped 0 mostly NaN tiles (> 100% nan)\n",
      "INFO:2022-04-15 15:50:38,709: Reverted any NaNs in remaining tiles to 0\n",
      "INFO:2022-04-15 15:50:38,709: None of these values should show NaN!\n",
      "INFO:2022-04-15 15:50:38,709: np.min(lr_patches)=-1.84499\n",
      "INFO:2022-04-15 15:50:38,709: np.max(lr_patches)=1.7417902\n",
      "INFO:2022-04-15 15:50:38,709: np.mean(lr_patches)=0.28155664\n",
      "INFO:2022-04-15 15:50:38,718: np.std(lr_patches)=0.34442413\n",
      "INFO:2022-04-15 15:50:39,011: (240, 240)\n",
      "INFO:2022-04-15 15:50:39,012: (120, 120)\n",
      "INFO:2022-04-15 15:50:39,012: Saved 133 tiles to D:\\luke\\data_train\n",
      "INFO:2022-04-15 15:50:39,018: Processing D:\\luke\\data_source\\surveys_line_spacing\\P738\\p738_3.ers, D:\\luke\\data_source\\surveys_line_spacing\\P738\\p738_1.ers\n",
      "INFO:2022-04-15 15:50:39,853: Dropped 0 mostly NaN tiles (> 100% nan)\n",
      "INFO:2022-04-15 15:50:39,860: Reverted any NaNs in remaining tiles to 0\n",
      "INFO:2022-04-15 15:50:39,860: None of these values should show NaN!\n",
      "INFO:2022-04-15 15:50:39,860: np.min(lr_patches)=-1.6722534\n",
      "INFO:2022-04-15 15:50:39,860: np.max(lr_patches)=1.826698\n",
      "INFO:2022-04-15 15:50:39,860: np.mean(lr_patches)=0.28186697\n",
      "INFO:2022-04-15 15:50:39,860: np.std(lr_patches)=0.34378153\n",
      "INFO:2022-04-15 15:50:42,080: (240, 240)\n",
      "INFO:2022-04-15 15:50:42,080: (80, 80)\n",
      "INFO:2022-04-15 15:50:42,080: Saved 133 tiles to D:\\luke\\data_train\n",
      "INFO:2022-04-15 15:50:42,094: Processing D:\\luke\\data_source\\surveys_line_spacing\\P738\\p738_4.ers, D:\\luke\\data_source\\surveys_line_spacing\\P738\\p738_1.ers\n",
      "INFO:2022-04-15 15:50:42,896: Dropped 0 mostly NaN tiles (> 100% nan)\n",
      "INFO:2022-04-15 15:50:42,908: Reverted any NaNs in remaining tiles to 0\n",
      "INFO:2022-04-15 15:50:42,909: None of these values should show NaN!\n",
      "INFO:2022-04-15 15:50:42,910: np.min(lr_patches)=-1.7867379\n",
      "INFO:2022-04-15 15:50:42,910: np.max(lr_patches)=1.4996029\n",
      "INFO:2022-04-15 15:50:42,911: np.mean(lr_patches)=0.28199044\n",
      "INFO:2022-04-15 15:50:42,912: np.std(lr_patches)=0.34269443\n",
      "INFO:2022-04-15 15:50:46,143: (240, 240)\n",
      "INFO:2022-04-15 15:50:46,144: (60, 60)\n",
      "INFO:2022-04-15 15:50:46,144: Saved 133 tiles to D:\\luke\\data_train\n",
      "INFO:2022-04-15 15:50:46,150: D:\\luke\\data_source\\surveys_line_spacing\\P739\n",
      "INFO:2022-04-15 15:50:46,151: Processing D:\\luke\\data_source\\surveys_line_spacing\\P739\\P739_2.ers, D:\\luke\\data_source\\surveys_line_spacing\\P739\\P739_1.ers\n",
      "INFO:2022-04-15 15:50:46,576: Dropped 0 mostly NaN tiles (> 100% nan)\n",
      "INFO:2022-04-15 15:50:46,581: Reverted any NaNs in remaining tiles to 0\n",
      "INFO:2022-04-15 15:50:46,582: None of these values should show NaN!\n",
      "INFO:2022-04-15 15:50:46,583: np.min(lr_patches)=-1.7410035\n",
      "INFO:2022-04-15 15:50:46,583: np.max(lr_patches)=2.124218\n",
      "INFO:2022-04-15 15:50:46,584: np.mean(lr_patches)=0.44716513\n",
      "INFO:2022-04-15 15:50:46,586: np.std(lr_patches)=0.37740213\n",
      "INFO:2022-04-15 15:50:46,700: (240, 240)\n",
      "INFO:2022-04-15 15:50:46,700: (120, 120)\n",
      "INFO:2022-04-15 15:50:46,700: Saved 49 tiles to D:\\luke\\data_train\n",
      "INFO:2022-04-15 15:50:46,705: Processing D:\\luke\\data_source\\surveys_line_spacing\\P739\\P739_3.ers, D:\\luke\\data_source\\surveys_line_spacing\\P739\\P739_1.ers\n",
      "INFO:2022-04-15 15:50:47,100: Dropped 0 mostly NaN tiles (> 100% nan)\n",
      "INFO:2022-04-15 15:50:47,105: Reverted any NaNs in remaining tiles to 0\n",
      "INFO:2022-04-15 15:50:47,106: None of these values should show NaN!\n",
      "INFO:2022-04-15 15:50:47,106: np.min(lr_patches)=-1.6632025\n",
      "INFO:2022-04-15 15:50:47,107: np.max(lr_patches)=2.1758301\n",
      "INFO:2022-04-15 15:50:47,108: np.mean(lr_patches)=0.45716622\n",
      "INFO:2022-04-15 15:50:47,109: np.std(lr_patches)=0.37386867\n",
      "INFO:2022-04-15 15:50:47,877: (240, 240)\n",
      "INFO:2022-04-15 15:50:47,877: (80, 80)\n",
      "INFO:2022-04-15 15:50:47,877: Saved 49 tiles to D:\\luke\\data_train\n",
      "INFO:2022-04-15 15:50:47,893: Processing D:\\luke\\data_source\\surveys_line_spacing\\P739\\P739_4.ers, D:\\luke\\data_source\\surveys_line_spacing\\P739\\P739_1.ers\n",
      "INFO:2022-04-15 15:50:48,257: Dropped 0 mostly NaN tiles (> 100% nan)\n",
      "INFO:2022-04-15 15:50:48,262: Reverted any NaNs in remaining tiles to 0\n",
      "INFO:2022-04-15 15:50:48,263: None of these values should show NaN!\n",
      "INFO:2022-04-15 15:50:48,263: np.min(lr_patches)=-1.6111228\n",
      "INFO:2022-04-15 15:50:48,264: np.max(lr_patches)=2.076676\n",
      "INFO:2022-04-15 15:50:48,265: np.mean(lr_patches)=0.46576327\n",
      "INFO:2022-04-15 15:50:48,265: np.std(lr_patches)=0.37005267\n",
      "INFO:2022-04-15 15:50:49,048: (240, 240)\n",
      "INFO:2022-04-15 15:50:49,049: (60, 60)\n",
      "INFO:2022-04-15 15:50:49,050: Saved 49 tiles to D:\\luke\\data_train\n"
     ]
    }
   ],
   "source": [
    "survey_search = \"P*\"\n",
    "train_dir = Path(r\"D:\\luke\\data_train\\qt_norm\")\n",
    "norm = Norm(r\"D:\\luke\\SRvey\\utils\\AUS_MAGPMAP_v7_ONSHORE_QuantileTransformer.joblib\")\n",
    "for survey_path in Path(r\"D:\\luke\\data_source\\surveys_line_spacing\").glob(survey_search):\n",
    "    if not survey_path.is_dir():\n",
    "        continue\n",
    "    logger.info(survey_path)\n",
    "\n",
    "    for scale in [2, 3, 4]:\n",
    "\n",
    "        root = survey_path\n",
    "        img_to_tiles(\n",
    "            hr_raster_path=f\"{next(root.glob('*1.ers'))}\",\n",
    "            lr_raster_path=f\"{next(root.glob(f'*{scale:d}.ers'))}\",\n",
    "            # hr_raster_path=next(survey_path.glob(\"*0200.tif\")),\n",
    "            # lr_raster_path=next(survey_path.glob(\"*0050.tif\")),\n",
    "            ext=\"tif\",\n",
    "            norm=norm.transform,\n",
    "            single_output_folder=train_dir,\n",
    "            nan_val=-999_999,\n",
    "            scale=scale,  # lr tile res\n",
    "            hr_s=240,  # hr tile res\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:2022-04-15 16:12:58,606: val_indices=[0, 22, 24, 32, 42, 46, 48, 72, 75, 80, 81, 87, 90, 103, 106, 108, 125, 135, 139, 145, 155, 157, 169, 176, 178, 179, 183, 194, 219, 229, 232, 251, 256, 257, 259, 263, 264, 265, 267, 273, 281, 289, 306, 308, 314, 326, 327, 330, 332, 339, 346, 353, 361, 363, 367, 369, 371, 378, 380, 382, 383, 407, 410]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_indices=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 23, 25, 26, 27, 28, 29, 30, 31, 33, 34, 35, 36, 37, 38, 39, 40, 41, 43, 44, 45, 47, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 73, 74, 76, 77, 78, 79, 82, 83, 84, 85, 86, 88, 89, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 104, 105, 107, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 126, 127, 128, 129, 130, 131, 132, 133, 134, 136, 137, 138, 140, 141, 142, 143, 144, 146, 147, 148, 149, 150, 151, 152, 153, 154, 156, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 170, 171, 172, 173, 174, 175, 177, 180, 181, 182, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 220, 221, 222, 223, 224, 225, 226, 227, 228, 230, 231, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 252, 253, 254, 255, 258, 260, 261, 262, 266, 268, 269, 270, 271, 272, 274, 275, 276, 277, 278, 279, 280, 282, 283, 284, 285, 286, 287, 288, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 307, 309, 310, 311, 312, 313, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 328, 329, 331, 333, 334, 335, 336, 337, 338, 340, 341, 342, 343, 344, 345, 347, 348, 349, 350, 351, 352, 354, 355, 356, 357, 358, 359, 360, 362, 364, 365, 366, 368, 370, 372, 373, 374, 375, 376, 377, 379, 381, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 408, 409, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421]\n",
      "There are 0 val indices in your train indices :)\n",
      "D:\\luke\\data_train\\hr240_qt processed and output to D:\\luke\\data_train\\hr240_qt\n"
     ]
    }
   ],
   "source": [
    "## ArbSR\n",
    "\n",
    "# ArbSR requires a specific dataset layout (see the matlab version of this script)\n",
    "# ArbSR uses a single HR file and n, m scale downsamplings.\n",
    "# We use a set of pre-gridded files, split into n directories of m scale.\n",
    "# This script organises these directories as per the expectations of ArbSR.\n",
    "# ArbSr uses np.arange(1.5, 4.5, 0.5), we have np.arange(2.0, 5.0, 1.0) == [2,3,4]\n",
    "\n",
    "# val_num = int(np.round(val_pct * len(lr)))  # len(lr_patches)\n",
    "# val_indices = sorted(rng.choice(len(lr), size=val_num, replace=False))\n",
    "# logger.info(f\"{val_indices=}\")\n",
    "\n",
    "# train_indices = [i for i in range(len(lr)) if i not in val_indices]\n",
    "# logger.info(f\"{train_indices=}\")\n",
    "# logger.info(\n",
    "#     f\"There are {sum(i in train_indices for i in val_indices)} val indices in your train indices :)\"\n",
    "# )\n",
    "\n",
    "\n",
    "def process_dir(\n",
    "    tile_dir,\n",
    "    out_dir_name=\"output\",\n",
    "    val_pct=0.15,  # 15% Val, 85% Train\n",
    "):\n",
    "    \"\"\"Based on dir name, process as n scale, for scale = \"dir_name_n\" \"\"\"\n",
    "\n",
    "    rng = np.random.default_rng(seed=21)\n",
    "\n",
    "    tile_dir = Path(tile_dir)\n",
    "    assert tile_dir.exists(), f\"Error, {tile_dir.absolute().as_posix()} not found!\"\n",
    "\n",
    "    num_tiles = len(list((tile_dir / \"hr\" / \"1-0\").iterdir()))\n",
    "    val_num = int(np.round(val_pct * num_tiles))  # len(lr_patches)\n",
    "    val_indices = sorted(rng.choice(num_tiles, size=val_num, replace=False))\n",
    "    logger.info(f\"{val_indices=}\")\n",
    "\n",
    "    train_indices = [i for i in range(num_tiles) if i not in val_indices]\n",
    "    print(f\"{train_indices=}\")\n",
    "    print(\n",
    "        f\"There are {sum(i in train_indices for i in val_indices)} val indices in your train indices :)\"\n",
    "    )\n",
    "\n",
    "    for d in (tile_dir / \"hr\").iterdir():\n",
    "        # This should only be 1 iteration, but matches lr loop for clarity\n",
    "        assert d.is_dir(), \"Unexpected files found\"\n",
    "        \n",
    "        hr_files = np.array(sorted(list(d.iterdir())))\n",
    "        \n",
    "        out_path_t = tile_dir / out_dir_name / \"train\" / \"HR\"\n",
    "        out_path_v = tile_dir / out_dir_name / \"val\" / \"HR\"\n",
    "        out_path_t.mkdir(exist_ok=True, parents=True)\n",
    "        out_path_v.mkdir(exist_ok=True, parents=True)\n",
    "        train_hr = hr_files[train_indices]\n",
    "        val_hr = hr_files[val_indices]\n",
    "\n",
    "        for i, f in enumerate(train_hr):\n",
    "            f.rename(out_path_t / f\"{i:04d}.tif\")\n",
    "        for i, f in enumerate(val_hr):\n",
    "            f.rename(out_path_v / f\"{i:04d}.tif\")\n",
    "\n",
    "    for d in (tile_dir / \"lr\").iterdir():\n",
    "        assert d.is_dir(), \"Unexpected files found\"\n",
    "\n",
    "        scale = float(d.stem.split(\"_x\")[-1].replace(\"-\", \".\"))\n",
    "    \n",
    "        lr_files = np.array(sorted(list(d.iterdir())))\n",
    "        \n",
    "        out_path_t = tile_dir / out_dir_name / \"train\" / f\"LR/X{scale:.2f}_X{scale:.2f}\"\n",
    "        out_path_v = tile_dir / out_dir_name / \"val\" / f\"LR/X{scale:.2f}_X{scale:.2f}\"\n",
    "        out_path_t.mkdir(exist_ok=True, parents=True)\n",
    "        out_path_v.mkdir(exist_ok=True, parents=True)\n",
    "        train_lr = lr_files[train_indices]\n",
    "        val_lr = lr_files[val_indices]\n",
    "\n",
    "        for i, f in enumerate(train_lr):\n",
    "            f.rename(out_path_t / f\"{i:04d}.tif\")\n",
    "        for i, f in enumerate(val_lr):\n",
    "            f.rename(out_path_v / f\"{i:04d}.tif\")\n",
    "\n",
    "    print(\n",
    "        f\"{tile_dir} processed and output to {(tile_dir).absolute()}\"\n",
    "    )\n",
    "\n",
    "\n",
    "tile_dir = r\"D:\\luke\\data_train\\hr240_qt\"#train_dir\n",
    "\n",
    "process_dir(tile_dir, out_dir_name=\"processed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import logging\n",
    "\n",
    "# logging.basicConfig(\n",
    "#     level=logging.INFO,\n",
    "#     format=\"%(levelname)s:%(asctime)s: %(message)s\",\n",
    "# )\n",
    "# logger = logging.getLogger(__name__)\n",
    "# logger.info(\"\\n#####\\nStarting new log\")\n",
    "\n",
    "# from pathlib import Path\n",
    "\n",
    "# import colorcet as cc\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "# import rasterio as rio\n",
    "# import torch\n",
    "# import tifffile\n",
    "# from PIL import Image\n",
    "\n",
    "\n",
    "# def raster_to_tiles(\n",
    "#     scale_raster_path_dict,\n",
    "#     lr_out_prefix=\"lr\",\n",
    "#     hr_out_prefix=\"hr\",\n",
    "#     hr_tile_dim=240,  # X1 scale factor tile dimension\n",
    "#     nan_val=-999_999,\n",
    "#     ext=\"tif\",  # output to n tif files, or numpy nd array\n",
    "#     norm=False,  # min max as below\n",
    "#     single_output_folder=False,  # Combine everything to target folder\n",
    "# ):\n",
    "\n",
    "#     \"\"\"\n",
    "#     So you have line data from a geophysical survey. You've decimated the lines\n",
    "#     and gridded it at several specific scale factors, e.g. remove 2nd, 3rd, 4th lines\n",
    "#     You now want to turn this survey into individual tiles for ML. So that each tile\n",
    "#     covers the same extent, you need to tile them at dimensions relative to their\n",
    "#     scale factor. The dimensions are therefore a decimal (or fractional) scale smaller\n",
    "#     than the original 1x scale grid.\n",
    "#     You may note that 256/3 is not a pleasant number for a discrete count of pixels.\n",
    "#     So we use 240, which goes to 120, 80, and 60 pixels per dimension for each of\n",
    "#                 1,                 2,  3, and  4 times scale, etc.\n",
    "\n",
    "#     An alternative would be to interpolate, but idk how that would affect ArbSR...\n",
    "#     Its easier in the image world, because it's just bicubic downsample on the fly.\n",
    "\n",
    "#     Normalisation is hacky and requires further work. It currently uses \"nice\"\n",
    "#     statistics from the Australia wide mag map, which is generally representative\n",
    "#     of TMI values, and constrains most data to [0,1]\n",
    "\n",
    "#     \"\"\"\n",
    "\n",
    "#     if norm:\n",
    "#         max_ = 1000  # Aus Magmap v7 histogram arbitrary clip/stats\n",
    "#         min_ = -1000\n",
    "#         mean_ = 0\n",
    "#         std_ = 250\n",
    "\n",
    "#         # norm = lambda i: (i + 4403.574) / 18454.907 # values from training\n",
    "#         # unnorm = lambda i: (i * 18454.907) - 4403.574\n",
    "\n",
    "#         norm = lambda i: (i - min_) / (max_ - min_)  # values from training\n",
    "#         unnorm = lambda i: (i * (max_ - min_)) + min_\n",
    "\n",
    "#         logger.info(f\"Using {max_=}, {min_=}, {mean_=}, {std_=} for min max norm\")\n",
    "\n",
    "#     else:\n",
    "#         norm = unnorm = lambda i: i  # NULL OP\n",
    "#         logger.info(\"Not Normalising ...\")\n",
    "\n",
    "#     for scale, raster_path in scale_raster_path_dict.items():\n",
    "#         logger.info(f\"Processing {raster_path} at scale {scale}\")\n",
    "#         raster_path = Path(raster_path)\n",
    "#         if raster_path.suffix == \".tif\":\n",
    "#             raster = tifffile.imread(raster_path)\n",
    "#         elif raster_path.suffix == \".ers\":\n",
    "#             raster = np.array(rio.open(raster_path).read(1))\n",
    "\n",
    "#         raster[raster == nan_val] = np.nan\n",
    "#         logger.debug(f\"{raster.shape}\")\n",
    "#         r_tensor = torch.as_tensor(norm(raster), dtype=torch.float32).unsqueeze(0)\n",
    "#         logger.debug(f\"{r_tensor.shape}\")\n",
    "\n",
    "#         # scale = float(scale.replace(\"-\", \".\"))\n",
    "#         sz = int(hr_tile_dim // scale)\n",
    "#         assert sz == hr_tile_dim / scale  # TODO handle fractional sizes\n",
    "#         logger.info(f\"Tiling scale factor {scale} at {sz}x{sz}\")\n",
    "\n",
    "#         # lr_original_extent = lr_tensor.shape\n",
    "\n",
    "#         # Expand raster size to a common multiple of lr size (which is a multiple of hr)\n",
    "\n",
    "#         # padded_w = (lr_tensor.shape[0] + (lr_s - (lr_tensor.shape[0] % lr_s)))\n",
    "#         # padded_h = (lr_tensor.shape[1] + (lr_s - (lr_tensor.shape[1] % lr_s)))\n",
    "#         logger.debug(f\"{repr(sz)=}\")\n",
    "#         logger.debug(f\"{repr(r_tensor.shape)=}\")\n",
    "#         logger.debug(f\"{repr((sz - (r_tensor.shape[0] % sz)))=}\")\n",
    "\n",
    "#         r_tensor = torch.nn.functional.pad(\n",
    "#             r_tensor,\n",
    "#             (\n",
    "#                 0,\n",
    "#                 (sz - (r_tensor.shape[1] % sz)),\n",
    "#                 0,\n",
    "#                 (sz - (r_tensor.shape[0] % sz)),\n",
    "#             ),\n",
    "#             mode=\"reflect\",\n",
    "#         )\n",
    "\n",
    "#         r_tensor = r_tensor[0]\n",
    "#         logger.debug(f\"{r_tensor.shape}\")\n",
    "\n",
    "#         # Math to determine and execute the tiling process\n",
    "#         tiles_per_row = r_tensor.shape[1] // sz\n",
    "#         tiles_per_column = r_tensor.shape[0] // sz\n",
    "\n",
    "#         r_patches = r_tensor.unfold(0, sz, sz).unfold(1, sz, sz)\n",
    "\n",
    "#         r_patches = r_patches.contiguous().view(\n",
    "#             tiles_per_row * tiles_per_column, -1, sz, sz\n",
    "#         )\n",
    "\n",
    "#         logger.debug(f\"{len(r_patches)=}\")\n",
    "#         logger.debug(f\"{r_patches.shape=}\")\n",
    "\n",
    "#         #  Drop tiles that contain mostly/any nan values, convert rest to some value\n",
    "#         allowed_nan_pct = 0.05\n",
    "#         nan_fill_val = mean_  # nan_val\n",
    "#         valid_mask = (\n",
    "#             torch.count_nonzero(r_patches.isnan(), dim=(2, 3)) / sz ** 2\n",
    "#         ) <= allowed_nan_pct\n",
    "\n",
    "#         r_patches_masked = r_patches[valid_mask]\n",
    "#         logger.debug(\n",
    "#             f\"Dropped {sum(~valid_mask).item()} mostly NaN tiles (> {allowed_nan_pct*100}% NaN)\"\n",
    "#         )\n",
    "\n",
    "#         r_patches_masked = torch.nan_to_num(r_patches_masked, nan=nan_fill_val)\n",
    "#         logger.debug(f\"Reverted any NaNs in remaining tiles to {nan_fill_val}\")\n",
    "\n",
    "#         # Random split the train/val tiles for this dataset\n",
    "#         from numpy.random import default_rng\n",
    "\n",
    "#         # rng = default_rng(seed=21)\n",
    "\n",
    "#         # val_pct = 0.15  # 15% Val, 85% Train\n",
    "#         # val_num = int(np.round(val_pct * len(r_patches_masked)))  # len(lr_patches)\n",
    "\n",
    "#         # val_indices = sorted(\n",
    "#         #     rng.choice(len(r_patches_masked), size=val_num, replace=False)\n",
    "#         # )\n",
    "#         # train_indices = [\n",
    "#         #     i for i in range(len(r_patches_masked)) if i not in val_indices\n",
    "#         # ]\n",
    "#         # logger.info(f\"{val_indices=}\")\n",
    "#         # logger.info(f\"{train_indices=}\")\n",
    "\n",
    "#         # r_patches_train = r_patches_masked[train_indices].numpy().astype(np.float32)\n",
    "#         # r_patches_val = r_patches_masked[val_indices].numpy().astype(np.float32)\n",
    "\n",
    "#         # logger.info(f\"{len(r_patches_train)=}\")\n",
    "#         # logger.info(f\"{len(r_patches_val)=}\")\n",
    "\n",
    "#         # logger.info(\"None of these values should show nans!\")\n",
    "#         # logger.info(f\"{np.max(r_patches_train)=}, {np.min(r_patches_train)=}\")\n",
    "#         # logger.info(f\"{np.mean(r_patches_train)=}, {np.std(r_patches_train)=}\")\n",
    "\n",
    "#         r_patches = r_patches_masked.numpy().astype(np.float32)\n",
    "\n",
    "#         if single_output_folder:\n",
    "#             single_output_folder = Path(single_output_folder)\n",
    "#             train_dir = single_output_folder / \"train\" / raster_path.stem\n",
    "#             # val_dir = single_output_folder / \"val\"\n",
    "#         else:\n",
    "#             train_dir = Path(raster_path).parent / ext / \"train\" / raster_path.stem\n",
    "#             # val_dir = Path(raster_path).parent / ext / \"val\"\n",
    "\n",
    "#         r_file_name = \"HR/\" if scale == 1 else (f\"LR/X{scale:.2f}_X{scale:.2f}/\")\n",
    "#         (train_dir / r_file_name).mkdir(parents=True, exist_ok=True)\n",
    "#         # (val_dir / r_file_name).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "#         # if \"npy\" in ext:\n",
    "#         #     np.save(train_dir / f\"{r_file_name}.{ext}\", r_patches_train)\n",
    "#         #     np.save(val_dir / f\"{r_file_name}.{ext}\", r_patches_val)\n",
    "\n",
    "#         j = len(list((train_dir / r_file_name).iterdir()))\n",
    "#         # h = len(list((val_dir / r_file_name).iterdir()))\n",
    "\n",
    "#         if \"tif\" in ext:\n",
    "#             for i in range(len(r_patches)):\n",
    "#                 tifffile.imsave(\n",
    "#                       / r_file_name / f\"{1+i+j:04d}.tif\", r_patches[i]\n",
    "#                 )\n",
    "\n",
    "#             # for i in range(len(r_patches_val)):\n",
    "#             #     tifffile.imsave(\n",
    "#             #         val_dir / r_file_name / f\"{1+i+h:04d}.tif\", r_patches_val[i]\n",
    "#             #     )\n",
    "\n",
    "#             # logger.info(r_patches[i].shape)\n",
    "\n",
    "#         logger.info(f\"\\nSaved to {train_dir.parent.absolute()}\\n\")\n",
    "\n",
    "#     return r_patches, train_indices, val_indices\n",
    "\n",
    "# survey_search = \"*.tif\"\n",
    "\n",
    "# survey_search = \"P*\"\n",
    "# survey_extension = \"*.ers\"\n",
    "# scale_raster_path_dict = {}\n",
    "# single_output_folder = Path(\"C:/Luke/data/Paper_2/lr64_combined\")\n",
    "\n",
    "# assert (\n",
    "#     len(list(single_output_folder.iterdir())) == 0\n",
    "# ), f\"dir {single_output_folder} is not empty!\"\n",
    "\n",
    "# for survey_path in Path(r\"C:\\Luke\\data\\Paper_2\").glob(survey_search):\n",
    "#     if not survey_path.is_dir():\n",
    "#         continue\n",
    "#     root = survey_path\n",
    "\n",
    "#     logging.info(f\"{survey_path.stem=}\")\n",
    "#     scale_raster_path_dict = scale_raster_path_dict | {survey_path.stem: {}}\n",
    "\n",
    "#     for scale_raster in root.glob(survey_extension):\n",
    "#         scale = float(scale_raster.stem.split(\"_\")[-1].replace(\"-\", \".\"))\n",
    "#         # > Python 3.9 dict merging with pipe operator\n",
    "#         scale_raster_path_dict[survey_path.stem] = scale_raster_path_dict[\n",
    "#             survey_path.stem\n",
    "#         ] | {scale: str(scale_raster.absolute())}\n",
    "\n",
    "#     r_patches, train_indices, val_indices = raster_to_tiles(\n",
    "#         scale_raster_path_dict=scale_raster_path_dict[survey_path.stem],\n",
    "#         ext=\"tif\",\n",
    "#         norm=True,\n",
    "#         single_output_folder=single_output_folder,\n",
    "#         nan_val=-999_999,\n",
    "#         hr_tile_dim=240,\n",
    "#     )\n",
    "\n",
    "# logger.info(scale_raster_path_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_tiles(data_path, index=0, ext=\"np\", s=256):\n",
    "    # if \"np\" in ext:\n",
    "    #     lr_tile = np.load(lr_path)[index][0]\n",
    "    #     hr_tile = np.load(hr_path)[index][0]\n",
    "    # elif \"tif\" in ext:\n",
    "    #     lr_tile = tifffile.imread(f\"{lr_path}\").squeeze()\n",
    "    #     hr_tile = tifffile.imread(f\"{hr_path}\").squeeze()\n",
    "    data_path = Path(data_path)\n",
    "\n",
    "    if \"tif\" in ext:\n",
    "        lr_tile = tifffile.imread(f\"{next(data_path.glob(f'**/lr/{i}.tif'))}\").squeeze()\n",
    "        hr_tile = tifffile.imread(f\"{next(data_path.glob(f'**/hr/{i}.tif'))}\").squeeze()\n",
    "\n",
    "    us = np.array(Image.fromarray(lr_tile).resize((s, s)))\n",
    "\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(us, vmin=hr_tile.min(), vmax=hr_tile.max())\n",
    "    plt.colorbar()\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.imshow(hr_tile)\n",
    "    plt.colorbar()\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.imshow(hr_tile - us, cmap=cc.cm.CET_D7, vmin=-0.5, vmax=0.5)\n",
    "    plt.colorbar()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def ers_to_tifs(survey_path, out_dir=\"\"):\n",
    "#     out_dir = Path(out_dir)\n",
    "#     out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "#     ers_arr = np.array(rio.open(survey_path).read(1))\n",
    "#     tifffile.imsave(out_dir / f\"{survey_path.stem}.tif\", ers_arr)\n",
    "#     print(f'Saved {Path(out_dir / f\"{survey_path.stem}.tif\").absolute()}')\n",
    "\n",
    "\n",
    "# survey_search = \"*4.ers\"\n",
    "# for survey_path in Path(\"C:/Luke/PhD/Oasis Montaj/ArbSR\").glob(survey_search):\n",
    "#     print(survey_path)\n",
    "#     # survey_path\n",
    "#     ers_to_tifs(survey_path, out_dir=\"PPDRC\")\n",
    "\n",
    "# # Test no loss of information:\n",
    "# # np.max(tifffile.imread(next(Path(r\"C:\\Luke\\PhD\\paper2\\SRvey\\utils\\PPDRC\").glob(\"*.tif\"))) - np.array(rio.open(r\"C:\\Luke\\PhD\\Oasis Montaj\\ArbSR\\p681_1.ers\").read(1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import tifffile\n",
    "import numpy as np\n",
    "\n",
    "ims = []\n",
    "for im in Path(r\"C:\\Luke\\data\\Paper 2\\PPDRC\\lr32\\train\").glob(\"**\\*.tif\"):\n",
    "    ims.append(tifffile.imread(im))\n",
    "\n",
    "ims = np.array(ims)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_search = \"**/*\"\n",
    "\n",
    "for survey_path in Path(r\"C:\\Luke\\data\\Paper 2\\PPDRC\\lr32\\train\").glob(survey_search):\n",
    "    logger.info(tifffile.imread(next(survey_path.glob(\"**/*0050.tif\"))).max())\n",
    "    logger.info(tifffile.imread(next(survey_path.glob(\"**/*0050.tif\"))).max())\n",
    "    logger.info(tifffile.imread(next(survey_path.glob(\"**/*0200.tif\"))).min())\n",
    "    logger.info(tifffile.imread(next(survey_path.glob(\"**/*0200.tif\"))).min())\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c2a7693b0d28a18d07a779b8850ec935428aaafb4510b5c22ddb8cee62302900"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('srvey': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
